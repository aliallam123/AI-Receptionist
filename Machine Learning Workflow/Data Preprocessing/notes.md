

'''
ğŸ”‘ 3. Turn Filtering

Decide which speaker turns you care about.

If you want your model to act like a receptionist:

Focus on SYSTEM turns (the answers).

Optionally keep USER turns as the context.
'''

''' ğŸ”‘ 7. Splitting & Balancing

Make sure you have proper train/validation/test splits.

Standard: 80/10/10 split (train/val/test).

Check class balance (e.g., not all examples are restaurant, some hotel too).
'''



'''
ğŸ”‘ 8. Feature Formatting

Convert your cleaned data into a format your model accepts:

For ML/Deep Learning â†’ often a .csv or .parquet with input_text and target_text.

For classical ML â†’ structured features (slots, intents, etc.).

Two possible directions:

Classical ML (baseline):

Input = TF-IDF features of user turns.

Output = intent/response label.

Neural Seq2Seq / Transformers (advanced):

Input = raw text (user history).

Output = raw text (system reply).

ğŸ‘‰ Save in .csv or HuggingFace dataset format.
'''

''' 
ğŸ”‘ 9. Sanity Checks

Make sure no empty texts.

Check that your domains and speakers make sense.

Spot-check 5â€“10 random dialogues manually to confirm preprocessing didnâ€™t break meaning.
'''

''' 
â³ How long should this take?

Steps 1â€“4: 2â€“3 focused sessions (~1 day).

Steps 5â€“6: Optional, adds ~2 days if you want slot handling.

Steps 7â€“9: 1 session (~half a day).

So preprocessing is about 3â€“5 days max if youâ€™re systematic. Thatâ€™s enough to get you a dataset you can actually model with, without falling into the â€œparadox of choice.â€
'''

Some useful pandas commands for EDA:

ğŸ” Exploring data

df.head(n) â†’ view first n rows

df.tail(n) â†’ view last n rows

df.shape â†’ (rows, columns)

df.info() â†’ column types & nulls

df.describe() â†’ summary stats for numeric columns

df.columns â†’ list of column names

df.dtypes â†’ data types of each column

ğŸ§¹ Filtering & selection

df["col"] â†’ select one column

df[["col1","col2"]] â†’ select multiple

df.loc[row_index, "col"] â†’ by label

df.iloc[row_number, col_number] â†’ by position

df[df["col"] == value] â†’ filter rows

df[df["col"].isin(["a","b"])] â†’ multiple matches

df.drop(columns=["col"]) â†’ remove columns

ğŸ§  Transforming

df["new"] = df["a"] + df["b"] â†’ create new column

df.rename(columns={"old":"new"}) â†’ rename

df.sort_values("col", ascending=False) â†’ sort

df.fillna(value) / df.dropna() â†’ handle missing values

df.apply(func) â†’ apply a custom function

ğŸ“Š Aggregating

df["col"].value_counts() â†’ frequency counts

df.groupby("col").mean() â†’ group and summarise

df.pivot_table(values="x", index="y", aggfunc="mean") â†’ pivot

ğŸ’¾ Import / export

pd.read_csv("file.csv") / df.to_csv("file.csv")

pd.read_json() / df.to_json()

pd.read_excel() / df.to_excel()